{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from utils import *\n",
    "from models import *\n",
    "import yaml\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Options:\n",
    "    def __init__(self):\n",
    "        self.model_name = 'alp'\n",
    "\n",
    "options = Options()\n",
    "options.output_file = 'test_output/reapos_test_text.txt'\n",
    "options.params_file = 'params-v1.yaml'\n",
    "options.use_cuda = False\n",
    "options.use_validation = True\n",
    "options.gpu2cpu = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-19 11:35:59,695 : INFO : loading Word2Vec object from /Users/alp/Documents/Corpora/OpenSubtitles2018/w2v/reapos_min5_heroes/w2v_en_heroes.model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use cuda: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-19 11:36:02,160 : INFO : loading wv recursively from /Users/alp/Documents/Corpora/OpenSubtitles2018/w2v/reapos_min5_heroes/w2v_en_heroes.model.wv.* with mmap=None\n",
      "2018-09-19 11:36:02,161 : INFO : loading vectors from /Users/alp/Documents/Corpora/OpenSubtitles2018/w2v/reapos_min5_heroes/w2v_en_heroes.model.wv.vectors.npy with mmap=None\n",
      "2018-09-19 11:36:02,354 : INFO : setting ignored attribute vectors_norm to None\n",
      "2018-09-19 11:36:02,357 : INFO : loading vocabulary recursively from /Users/alp/Documents/Corpora/OpenSubtitles2018/w2v/reapos_min5_heroes/w2v_en_heroes.model.vocabulary.* with mmap=None\n",
      "2018-09-19 11:36:02,362 : INFO : loading trainables recursively from /Users/alp/Documents/Corpora/OpenSubtitles2018/w2v/reapos_min5_heroes/w2v_en_heroes.model.trainables.* with mmap=None\n",
      "2018-09-19 11:36:02,369 : INFO : loading syn1neg from /Users/alp/Documents/Corpora/OpenSubtitles2018/w2v/reapos_min5_heroes/w2v_en_heroes.model.trainables.syn1neg.npy with mmap=None\n",
      "2018-09-19 11:36:02,550 : INFO : setting ignored attribute cum_table to None\n",
      "2018-09-19 11:36:02,556 : INFO : loaded /Users/alp/Documents/Corpora/OpenSubtitles2018/w2v/reapos_min5_heroes/w2v_en_heroes.model\n",
      "2018-09-19 11:36:03,304 : INFO : loading Word2Vec object from /Users/alp/Documents/Corpora/OpenSubtitles2018/w2v/reapos_min5_heroes/w2v_es_heroes.model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en Vocabulary size: 30000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-19 11:36:04,601 : INFO : loading wv recursively from /Users/alp/Documents/Corpora/OpenSubtitles2018/w2v/reapos_min5_heroes/w2v_es_heroes.model.wv.* with mmap=None\n",
      "2018-09-19 11:36:04,602 : INFO : loading vectors from /Users/alp/Documents/Corpora/OpenSubtitles2018/w2v/reapos_min5_heroes/w2v_es_heroes.model.wv.vectors.npy with mmap=None\n",
      "2018-09-19 11:36:04,902 : INFO : setting ignored attribute vectors_norm to None\n",
      "2018-09-19 11:36:04,907 : INFO : loading vocabulary recursively from /Users/alp/Documents/Corpora/OpenSubtitles2018/w2v/reapos_min5_heroes/w2v_es_heroes.model.vocabulary.* with mmap=None\n",
      "2018-09-19 11:36:04,914 : INFO : loading trainables recursively from /Users/alp/Documents/Corpora/OpenSubtitles2018/w2v/reapos_min5_heroes/w2v_es_heroes.model.trainables.* with mmap=None\n",
      "2018-09-19 11:36:04,917 : INFO : loading syn1neg from /Users/alp/Documents/Corpora/OpenSubtitles2018/w2v/reapos_min5_heroes/w2v_es_heroes.model.trainables.syn1neg.npy with mmap=None\n",
      "2018-09-19 11:36:05,302 : INFO : setting ignored attribute cum_table to None\n",
      "2018-09-19 11:36:05,309 : INFO : loaded /Users/alp/Documents/Corpora/OpenSubtitles2018/w2v/reapos_min5_heroes/w2v_es_heroes.model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "es Vocabulary size: 30000\n"
     ]
    }
   ],
   "source": [
    "#LOAD CONFIGURATIONS AND LANGUAGES\n",
    "USE_CUDA = options.use_cuda\n",
    "print(\"Use cuda: %s\" %USE_CUDA)\n",
    "\n",
    "try:\n",
    "    with open(options.params_file, 'r') as ymlfile:\n",
    "        config = yaml.load(ymlfile)\n",
    "except:\n",
    "    sys.exit(\"Parameters file missing\")\n",
    "\n",
    "#Setup languages\n",
    "INPUT_LANG_CODE = config['INPUT_LANG']\n",
    "OUTPUT_LANG_CODE = config['OUTPUT_LANG']\n",
    "\n",
    "INPUT_LANG_PUNC_LEVEL = config[\"INPUT_LANG_PUNC_LEVEL\"]\n",
    "OUTPUT_LANG_PUNC_LEVEL = config[\"OUTPUT_LANG_PUNC_LEVEL\"]\n",
    "\n",
    "if INPUT_LANG_CODE == 'en' and OUTPUT_LANG_CODE == 'es':\n",
    "    lang_en = input_lang = Lang(INPUT_LANG_CODE, config[\"W2V_EN_PATH\"], config[\"DICT_EN_PATH\"], punctuation_level=INPUT_LANG_PUNC_LEVEL)\n",
    "    lang_es = output_lang = Lang(OUTPUT_LANG_CODE, config[\"W2V_ES_PATH\"], config[\"DICT_ES_PATH\"], punctuation_level=OUTPUT_LANG_PUNC_LEVEL)\n",
    "elif INPUT_LANG_CODE == 'es' and OUTPUT_LANG_CODE == 'en':\n",
    "    lang_es = input_lang = Lang(INPUT_LANG_CODE, config[\"W2V_ES_PATH\"], config[\"DICT_ES_PATH\"], punctuation_level=INPUT_LANG_PUNC_LEVEL)\n",
    "    lang_en = output_lang = Lang(OUTPUT_LANG_CODE, config[\"W2V_EN_PATH\"], config[\"DICT_EN_PATH\"], punctuation_level=OUTPUT_LANG_PUNC_LEVEL)\n",
    "\n",
    "input_prosody_params = config['INPUT_PROSODY']\n",
    "if input_prosody_params == None:\n",
    "    input_prosody_params = []\n",
    "output_prosody_params = config['OUTPUT_PROSODY']\n",
    "if output_prosody_params == None:\n",
    "    output_prosody_params = []    \n",
    "    \n",
    "#NETWORK CONFIG\n",
    "max_seq_length = int(config['MAX_SEQ_LENGTH'])\n",
    "n_prosody_params = int(config['N_PROSODY_PARAMS'])\n",
    "input_prosody_params = config['INPUT_PROSODY']\n",
    "encoder_type = config['ENCODER_TYPE']\n",
    "attn_model = config['ATTN_MODEL']\n",
    "hidden_size = int(config['HIDDEN_SIZE'])\n",
    "n_layers = int(config['N_LAYERS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD DATASETS PATHS   \n",
    "AUDIO_TEST_DATA_PATH = config[\"AUDIO_TEST_DATA_FILE\"]\n",
    "AUDIO_TRAIN_DATA_PATH = config[\"AUDIO_TRAIN_DATA_FILE\"]\n",
    "AUDIO_VALIDATION_DATA_PATH = config[\"AUDIO_VALIDATION_DATA_FILE\"]\n",
    "TEXT_TEST_DATA_PATH = config['TEXT_TEST_DATA_PATH']\n",
    "AUDIO_ALL_DATA_PATH = \"/Users/alp/Movies/heroes/transProse_data/audiodata-v1/transProse_audiodata.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input punc lvl:  0\n",
      "Output punc lvl:  2\n",
      "gpu2cpu: True\n"
     ]
    }
   ],
   "source": [
    "#Initialize text models\n",
    "text_encoder_path = 'models/5mm_unpuncdinput_encoder.model'\n",
    "text_decoder_path = 'models/5mm_unpuncdinput_decoder.model'\n",
    "\n",
    "print('Input punc lvl: ', INPUT_LANG_PUNC_LEVEL)\n",
    "print('Output punc lvl: ', OUTPUT_LANG_PUNC_LEVEL)\n",
    "\n",
    "text_encoder = GenericEncoder(input_lang.vocabulary_size, hidden_size, input_lang.get_weights_matrix(), n_layers)\n",
    "text_decoder = LuongAttnDecoderRNN(attn_model, hidden_size, output_lang.get_weights_matrix(), output_lang.vocabulary_size, n_layers, input_feed=config['DECODER_INPUT_FEED'])\n",
    "load_model(text_encoder, text_decoder, text_encoder_path, text_decoder_path, options.gpu2cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu2cpu: True\n"
     ]
    }
   ],
   "source": [
    "#Initialize prosodic models\n",
    "AUDIO_ENCODE_ONLY = False\n",
    "model_name = \"audio_continousin\"\n",
    "prosodic_encoder_path = 'models/' + model_name + '_encoder.model'\n",
    "prosodic_decoder_path = 'models/' + model_name + '_decoder.model'\n",
    "\n",
    "if encoder_type == 'sum':\n",
    "    prosodic_encoder = EncoderRNN_sum_ver(input_lang.vocabulary_size, n_prosody_params, hidden_size, input_lang.get_weights_matrix(), n_layers)\n",
    "elif encoder_type == 'parallel':\n",
    "    prosodic_encoder = EncoderRNN_parallel(input_lang.vocabulary_size, n_prosody_params, hidden_size, input_lang.get_weights_matrix(), n_layers)\n",
    "else:\n",
    "    sys.exit(\"Unrecognized encoder type. Check params file. Exiting...\")\n",
    "if AUDIO_ENCODE_ONLY:\n",
    "    prosodic_decoder = LuongAttnDecoderRNN(attn_model, hidden_size, output_lang.get_weights_matrix(), output_lang.vocabulary_size, n_layers, USE_CUDA=USE_CUDA)\n",
    "else:\n",
    "    prosodic_decoder = ProsodicDecoderRNN(attn_model, hidden_size, output_lang.get_weights_matrix(), output_lang.vocabulary_size, n_layers, USE_CUDA=USE_CUDA)\n",
    "    \n",
    "load_model(prosodic_encoder, prosodic_decoder, prosodic_encoder_path, prosodic_decoder_path, gpu_to_cpu=options.gpu2cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA GENERATORS\n",
    "#generates data from tab separated file\n",
    "def text_data_generator(data_path, input_lang, output_lang, stop_at=-1):\n",
    "    count = 0\n",
    "    with open(data_path,'r') as inputfile:\n",
    "        for line in inputfile:\n",
    "            if not stop_at == -1 and count >= stop_at:\n",
    "                break\n",
    "            pair = [sentence.strip() for sentence in line.split('\\t')]\n",
    "            if input_lang.lang_code == 'en':\n",
    "                in_sentence = pair[0]\n",
    "                out_sentence = pair[1]\n",
    "            elif input_lang.lang_code == 'es':\n",
    "                in_sentence = pair[1]\n",
    "                out_sentence = pair[0]\n",
    "\n",
    "            in_sentence_tokens = in_sentence.lower().split()\n",
    "            out_sentence_tokens = out_sentence.lower().split()\n",
    "\n",
    "            if input_lang.punctuation_level == 0:\n",
    "                in_sentence_tokens = remove_punc_tokens(in_sentence_tokens)\n",
    "            elif input_lang.punctuation_level == 1:\n",
    "                in_sentence_tokens = remove_punc_tokens(in_sentence_tokens, keep_main_puncs=True)\n",
    "            if output_lang.punctuation_level == 0:\n",
    "                out_sentence_tokens = remove_punc_tokens(out_sentence_tokens)\n",
    "            elif output_lang.punctuation_level == 1:\n",
    "                out_sentence_tokens = remove_punc_tokens(out_sentence_tokens, keep_main_puncs=True)\n",
    "\n",
    "            count += 1\n",
    "\n",
    "            yield in_sentence_tokens, out_sentence_tokens\n",
    "\n",
    "def audio_data_generator(data_path, input_lang, output_lang, n_prosody_params, input_prosody_params, output_prosody_params, dummyfy_input_prosody=False, dummyfy_output_prosody=False, stop_at = -1):\n",
    "    assert not input_lang == output_lang\n",
    "    audio_data = read_audio_dataset_file(data_path, shuffle=False)\n",
    "\n",
    "    #start generating samples from the proscript links in the data file\n",
    "    count = 0\n",
    "    for segment_data in audio_data:\n",
    "        if not stop_at == -1 and count >= stop_at:\n",
    "            break\n",
    "\n",
    "        es_txt = segment_data[0]\n",
    "        es_csv = segment_data[1]\n",
    "        en_txt = segment_data[2]\n",
    "        en_csv = segment_data[3]\n",
    "        \n",
    "        #print(en_csv)\n",
    "        #print(es_csv)\n",
    "\n",
    "        if input_lang.lang_code == 'en' and output_lang.lang_code == 'es':\n",
    "            input_proscript = en_csv\n",
    "            output_proscript = es_csv\n",
    "            #input_transcript = read_text_file(en_txt)\n",
    "            #output_transcript = read_text_file(es_txt)\n",
    "        elif input_lang.lang_code == 'es' and output_lang.lang_code == 'en':\n",
    "            input_proscript = es_csv\n",
    "            output_proscript = en_csv\n",
    "            #input_transcript = read_text_file(es_txt)\n",
    "            #output_transcript = read_text_file(en_txt)\n",
    "            \n",
    "        if input_lang.punctuation_level == 0:\n",
    "            input_punc = False\n",
    "            input_only_main_punc = False\n",
    "        elif input_lang.punctuation_level == 1:\n",
    "            input_punc = True\n",
    "            input_only_main_punc = True\n",
    "        elif input_lang.punctuation_level == 2:\n",
    "            input_punc = True\n",
    "            input_only_main_punc = False\n",
    "            \n",
    "        if output_lang.punctuation_level == 0:\n",
    "            output_punc = False\n",
    "            output_only_main_punc = False\n",
    "        elif output_lang.punctuation_level == 1:\n",
    "            output_punc = True\n",
    "            output_only_main_punc = True\n",
    "        elif output_lang.punctuation_level == 2:\n",
    "            output_punc = True\n",
    "            output_only_main_punc = False\n",
    "\n",
    "        in_sentence_tokens, in_prosody_tokens = read_data_from_proscript(input_proscript, input_lang, n_prosody_params, input_prosody_params, punctuation_as_tokens = input_punc, keep_only_main_puncs = input_only_main_punc)\n",
    "        out_sentence_tokens, out_prosody_tokens = read_data_from_proscript(output_proscript, output_lang, n_prosody_params, output_prosody_params, punctuation_as_tokens = output_punc, keep_only_main_puncs = output_only_main_punc)\n",
    "    \n",
    "        if dummyfy_input_prosody:\n",
    "            in_prosody_tokens = np.zeros_like(in_prosody_tokens)\n",
    "        if dummyfy_output_prosody:\n",
    "            out_prosody_tokens = np.zeros_like(out_prosody_tokens)\n",
    "            \n",
    "        count += 1\n",
    "        yield in_sentence_tokens, in_prosody_tokens, out_sentence_tokens, out_prosody_tokens, en_csv, es_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#various utilities\n",
    "def print_prosody(prosody):\n",
    "    print(np.array(prosody).transpose())\n",
    "    \n",
    "def print_tokens_with_pause(tokens, pausevals = [], pauseflags=[]):\n",
    "    if pauseflags == [] and not pausevals == []:\n",
    "        pauseflags = flags_from_value(pausevals)\n",
    "        \n",
    "    to_print = \"\"\n",
    "    for i, token in enumerate(tokens):\n",
    "        if not token == \"END\":\n",
    "            to_print += token + \" \"\n",
    "            if not pauseflags == [] and pauseflags[i]:\n",
    "                to_print += \"[P]\"\n",
    "                if not pausevals == []:\n",
    "                    to_print += \"<%0.3f>\"%pausevals[i]\n",
    "                to_print += \" \"\n",
    "            \n",
    "    #print(to_print)\n",
    "    return to_print\n",
    "            \n",
    "def flags_from_value(prosody_seq):\n",
    "    return [1 if not feature_value == 0.0 else 0 for feature_value in prosody_seq]\n",
    "\n",
    "def show_attention(input_sentence, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') + ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SEE TEXT DATA\n",
    "for in_sent, out_sent in text_data_generator(TEXT_TEST_DATA_PATH, input_lang, output_lang):\n",
    "    print(in_sent)\n",
    "    print(out_sent)\n",
    "    exit = input('...')\n",
    "    if exit == 'q':\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'could', 'move', 'objects', 'with', 'my', 'mind', '.', 'i', 'could', '...']\n",
      "[3, 102, 317, 5067, 35, 25, 277, 0, 3, 102, 18, 29998]\n",
      "[[0.   0.   0.   0.   0.   0.   0.13 0.   0.   0.   0.   0.  ]]\n",
      "['podía', 'mover', 'objetos', 'con', 'mi', 'mente', ',', 'podía', '...']\n",
      "[[0.   0.   0.   0.   0.   0.15 0.   0.   0.  ]]\n",
      "...\n",
      "['what', 'makes', 'you', 'so', 'sure', 'it', \"'s\", 'a', 'boy', '?']\n",
      "[23, 411, 2, 45, 154, 10, 9, 8, 279, 5, 29998]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "['¿', 'y', 'por', 'qué', 'crees', 'que', 'es', 'un', 'chico', '?']\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "...Q\n",
      "['you', 'want', 'payback', ',', 'i', 'understand', 'that', ',', 'but', 'not', 'all', 'of', 'these', 'individuals', 'out', 'there', 'are', 'dangerous', '.', 'some', 'of', 'them', 'are', 'just', 'plain', 'scared', '.', 'and', 'now', 'that', 'they', 'know', 'they', \"'re\", 'being', 'pursued', ',', 'they', \"'re\", 'going', 'to', 'be', 'even', 'that', 'much', 'harder', 'to', 'catch', '.']\n",
      "[2, 68, 8036, 1, 3, 276, 12, 1, 36, 33, 42, 14, 141, 6719, 54, 47, 40, 924, 0, 100, 14, 93, 40, 41, 3093, 662, 0, 11, 62, 12, 46, 38, 46, 32, 215, 11515, 1, 46, 32, 87, 6, 31, 137, 12, 144, 1876, 6, 665, 0, 29998]\n",
      "[[0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.11 0.   0.   0.   0.   0.   0.   0.   0.39 0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.  ]]\n",
      "['y', 'quieren', 'venganza', ',', 'lo', 'comprendo', '.', 'pero', 'no', 'todos', 'estos', 'individuos', 'son', 'peligrosos', '.', 'algunos', 'sólo', 'tienen', 'miedo', 'y', 'ahora', 'que', 'saben', 'que', 'los', 'perseguimos', 'serán', 'más', 'difíciles', 'de', 'atrapar', '.']\n",
      "[[0.64 0.   0.11 0.   0.   0.52 0.   0.   0.05 0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   1.12 0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.  ]]\n",
      "...q\n"
     ]
    }
   ],
   "source": [
    "#SEE AUDIO DATA\n",
    "in_pros = []\n",
    "for in_sent, in_pros, out_sent, out_pros, in_csv, out_csv in audio_data_generator(AUDIO_TEST_DATA_PATH, input_lang, output_lang, n_prosody_params, input_prosody_params, output_prosody_params, dummyfy_input_prosody=False, dummyfy_output_prosody=False):\n",
    "    print(in_sent)\n",
    "    print(indexes_from_tokens(input_lang, in_sent))\n",
    "    in_pros = finalize_prosody_sequence(in_pros)\n",
    "    print_prosody(in_pros)\n",
    "    print(out_sent)\n",
    "    print_prosody(out_pros)\n",
    "    exit = input('...')\n",
    "    if exit == 'q':\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EVALUATORS\n",
    "#text -> text\n",
    "def evaluate_text(input_seq_tokens, input_lang, output_lang, encoder, decoder, max_length, USE_CUDA=False):\n",
    "    input_word_seqs = [indexes_from_tokens(input_lang, input_seq_tokens)]\n",
    "    \n",
    "    #make sure sequences are below max_length. \n",
    "    input_word_seqs = limit_seqs_to_max(input_word_seqs, max_length)\n",
    "\n",
    "    input_lengths = [len(input_word_seqs[0])]\n",
    "    input_word_batch = Variable(torch.LongTensor(input_word_seqs)).transpose(0, 1)\n",
    "\n",
    "    if USE_CUDA:\n",
    "        input_word_batch = input_batch.cuda()\n",
    "        input_prosody_batch = input_prosody_batch.cuda()\n",
    "\n",
    "    # Set to not-training mode to disable dropout\n",
    "    encoder.train(False)\n",
    "    decoder.train(False)\n",
    "\n",
    "    # Run through encoder\n",
    "    encoder_outputs, encoder_hidden = encoder(input_word_batch, input_lengths, None)\n",
    "\n",
    "    # Create starting vectors for decoder\n",
    "    decoder_input = Variable(torch.LongTensor([output_lang.token2index(SWT_TOKEN)])) # SOS\n",
    "    decoder_hidden = encoder_hidden[:decoder.n_layers] # Use last (forward) hidden state from encoder\n",
    "    decoder_context = Variable(torch.zeros(1, decoder.hidden_size))\n",
    "\n",
    "    if USE_CUDA:\n",
    "        decoder_input = decoder_input.cuda()\n",
    "        decoder_context = decoder_context.cuda()\n",
    "\n",
    "    # Store output words and attention states\n",
    "    decoded_words = []\n",
    "    decoder_attentions = torch.zeros(max_length + 1, max_length + 1)\n",
    "\n",
    "    # Run through decoder\n",
    "    for di in range(max_length):\n",
    "        decoder_output, decoder_context, decoder_hidden, decoder_attn = decoder(\n",
    "                decoder_input, decoder_context, decoder_hidden, encoder_outputs)\n",
    "        \n",
    "        decoder_attentions[di,:decoder_attn.size(2)] += decoder_attn.squeeze(0).squeeze(0).cpu().data\n",
    "\n",
    "        # Choose top word from output\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        #ni = topi[0][0]  #old code\n",
    "        ni = topi.item()\n",
    "        if ni == output_lang.token2index(EOS_TOKEN):\n",
    "            decoded_words.append(EOS_TOKEN)\n",
    "            break\n",
    "        else:\n",
    "            decoded_words.append(output_lang.index2token(ni))\n",
    "\n",
    "        # Next input is chosen word\n",
    "        decoder_input = Variable(torch.LongTensor([ni]))\n",
    "        if USE_CUDA: decoder_input = decoder_input.cuda()\n",
    "\n",
    "    # Set back to training mode\n",
    "    encoder.train(True)\n",
    "    decoder.train(True)\n",
    "\n",
    "    return decoded_words, decoder_attentions[:di+1, :len(encoder_outputs)]\n",
    "\n",
    "#text+audio -> text+audio\n",
    "#use prosodic encoder/decoder\n",
    "def evaluate_audio(input_seq_tokens, input_prosody_tokens, input_lang, output_lang, encoder, decoder, max_length, audio_encode_only, USE_CUDA=False):\n",
    "    input_word_seqs = [indexes_from_tokens(input_lang, input_seq_tokens)]\n",
    "    input_prosody_seqs = [finalize_prosody_sequence(input_prosody_tokens)] #put the end token\n",
    "    \n",
    "    #make sure sequences are below max_length. \n",
    "    input_word_seqs = limit_seqs_to_max(input_word_seqs, max_length)\n",
    "    input_prosody_seqs = limit_seqs_to_max(input_prosody_seqs, max_length)\n",
    "\n",
    "    input_lengths = [len(input_word_seqs[0])]\n",
    "    input_word_batch = Variable(torch.LongTensor(input_word_seqs)).transpose(0, 1)\n",
    "    input_prosody_batch = Variable(torch.FloatTensor(input_prosody_seqs)).transpose(0, 1)\n",
    "    \n",
    "    if USE_CUDA:\n",
    "        input_word_batch = input_batch.cuda()\n",
    "        input_prosody_batch = input_prosody_batch.cuda()\n",
    "\n",
    "    # Set to not-training mode to disable dropout\n",
    "    encoder.train(False)\n",
    "    decoder.train(False)\n",
    "\n",
    "    # Run through encoder\n",
    "    encoder_outputs, encoder_hidden = encoder(input_word_batch, input_prosody_batch, input_lengths, None)\n",
    "\n",
    "    # Create starting vectors for decoder\n",
    "    decoder_word_input = Variable(torch.LongTensor([output_lang.token2index(SWT_TOKEN)])) # SOS\n",
    "    decoder_hidden = encoder_hidden[:decoder.n_layers] # Use last (forward) hidden state from encoder\n",
    "    decoder_context = Variable(torch.zeros(1, decoder.hidden_size))\n",
    "\n",
    "    if USE_CUDA:\n",
    "        decoder_word_input = decoder_word_input.cuda()\n",
    "        decoder_context = decoder_context.cuda()\n",
    "\n",
    "    # Store output words and attention states\n",
    "    decoded_word_seq = []\n",
    "    decoded_pauseflag_seq = []\n",
    "    decoded_pausevalue_seq = []\n",
    "    decoder_attentions = torch.zeros(max_length + 1, max_length + 1)\n",
    "\n",
    "    # Run through decoder\n",
    "    decoder_stop = False\n",
    "    for di in range(max_length):\n",
    "        if audio_encode_only:\n",
    "            decoder_output_word, decoder_context, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_word_input, decoder_context, decoder_hidden, encoder_outputs)\n",
    "        else:\n",
    "#             decoder_output_word, decoder_output_pauseflag, decoder_output_pausevalue, decoder_context, decoder_hidden, decoder_attention = decoder(\n",
    "#                 decoder_word_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "#             )  #FLAGTEST\n",
    "            \n",
    "            decoder_output_word, decoder_output_pauseflag, decoder_context, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_word_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "        \n",
    "        decoder_attentions[di,:decoder_attention.size(2)] += decoder_attention.squeeze(0).squeeze(0).cpu().data\n",
    "            \n",
    "        # Choose top word from output\n",
    "        topv, topi = decoder_output_word.data.topk(1)\n",
    "        ni_word = topi.item()\n",
    "        if ni_word == output_lang.token2index(EOS_TOKEN):\n",
    "            decoded_word_seq.append(EOS_TOKEN)\n",
    "            decoder_stop = True\n",
    "        else:\n",
    "            decoded_word_seq.append(output_lang.index2token(ni_word))\n",
    "        \n",
    "        if not audio_encode_only:\n",
    "            # Look at pauseflag output\n",
    "            topv, topi = decoder_output_pauseflag.data.topk(1)\n",
    "            ni_pauseflag = topi.item()\n",
    "            decoded_pauseflag_seq.append(ni_pauseflag)\n",
    "\n",
    "            # Look at pauseval output\n",
    "            #predicted_pausevalue = decoder_output_pausevalue.item()\n",
    "            #decoded_pausevalue_seq.append(unnormalize_value(predicted_pausevalue, 0.0, 10.0))\n",
    "            #decoded_pausevalue_seq.append(predicted_pausevalue)\n",
    "\n",
    "        # Next input is chosen word\n",
    "        if decoder_stop:\n",
    "            break\n",
    "        else:\n",
    "            decoder_word_input = Variable(torch.LongTensor([ni_word]))\n",
    "            if USE_CUDA: decoder_word_input = decoder_word_input.cuda()\n",
    "\n",
    "    # Set back to training mode\n",
    "    encoder.train(True)\n",
    "    decoder.train(True)\n",
    "\n",
    "    #return decoded_word_seq, decoded_pauseflag_seq, decoded_pausevalue_seq, decoder_attentions[:di+1, :len(encoder_outputs)]  #FLAGTEST\n",
    "    return decoded_word_seq, decoded_pauseflag_seq, decoder_attentions[:di+1, :len(encoder_outputs)]  #FLAGTEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text -> text evaluator test\n",
    "input_sentence = \"he 's flying through that glass ?\" \n",
    "input_seq_tokens = input_sentence.split()\n",
    "decoded_words, attentions = evaluate_text(input_seq_tokens, input_lang, output_lang, text_encoder, text_decoder, max_seq_length)\n",
    "print(readable_from_tokens(decoded_words[:-1]))\n",
    "show_attention(input_sentence, decoded_words, attentions)\n",
    "#print(attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show_attention(readable_from_tokens(in_sentence_tokens), translation_tokens, attentions)\n",
    "print(attentions.shape)\n",
    "print(attentions[0])\n",
    "print(np.argmax(attentions[0]).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = audio_data_generator(AUDIO_TEST_DATA_PATH, input_lang, output_lang, n_prosody_params, input_prosody_params, output_prosody_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alp/extSW/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel/__main__.py:6: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n",
      "/Users/alp/extSW/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel/__main__.py:15: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n",
      "/Users/alp/extSW/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel/__main__.py:6: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n"
     ]
    }
   ],
   "source": [
    "#Text+prosody -> text+prosody on audio data visualization\n",
    "print_every = 0\n",
    "print_count = 0\n",
    "print_strings_for_prosody_evaluation = []\n",
    "for in_sentence_tokens, in_prosody_tokens, out_sentence_tokens, out_prosody_tokens, in_csv, out_csv in gen:\n",
    "    #print(in_csv)\n",
    "    #print(out_csv)\n",
    "    \n",
    "    in_string = print_tokens_with_pause(in_sentence_tokens, in_prosody_tokens[:,0])\n",
    "    gt_string = print_tokens_with_pause(out_sentence_tokens, out_prosody_tokens[:,0])\n",
    "    \n",
    "    #translation_tokens, pauseflag_tokens, pausevalue_tokens, _ = evaluate_audio(in_sentence_tokens, in_prosody_tokens, input_lang, output_lang, prosodic_encoder, prosodic_decoder, max_seq_length, audio_encode_only=AUDIO_ENCODE_ONLY)\n",
    "    translation_tokens, pauseflag_tokens, attentions = evaluate_audio(in_sentence_tokens, in_prosody_tokens, input_lang, output_lang, prosodic_encoder, prosodic_decoder, max_seq_length, audio_encode_only=AUDIO_ENCODE_ONLY)\n",
    "    prosody_translation_string = print_tokens_with_pause(translation_tokens, pauseflags=pauseflag_tokens)\n",
    "    \n",
    "    translation_tokens, _ = evaluate_text(in_sentence_tokens, input_lang, output_lang, text_encoder, text_decoder, max_seq_length)\n",
    "    text_translation_string = print_tokens_with_pause(translation_tokens)\n",
    "    \n",
    "    print_string = '%s\\t%s\\t%s'%(in_string,gt_string,prosody_translation_string)\n",
    "    print_strings_for_prosody_evaluation.append(print_string)\n",
    "    \n",
    "    print_count += 1\n",
    "    if print_every > 0 and  print_count % print_every == 0:\n",
    "        print(\"IN\")\n",
    "        print(in_string)\n",
    "        print(\"GT\")\n",
    "        print(gt_string)\n",
    "        print(\"OUT PROSODY\")\n",
    "        print(prosody_translation_string)\n",
    "        print(\"OUT TEXT\")\n",
    "        print(text_translation_string)\n",
    "        \n",
    "        inp = input(\"...\")\n",
    "        if inp == 'q':\n",
    "            break\n",
    "        print(\"======================================================================\")\n",
    "    \n",
    "with open('prosody_eval.txt', 'w') as f:\n",
    "    for eval_str in print_strings_for_prosody_evaluation:\n",
    "        f.write(eval_str+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text -> text on text data\n",
    "def text_set_translation_generator(stop_at = -1, report=False):\n",
    "    for in_sentence_tokens, out_sentence_tokens in text_data_generator(TEXT_TEST_DATA_PATH, input_lang, output_lang, stop_at):\n",
    "        translation_tokens, _ = evaluate_text(in_sentence_tokens, input_lang, output_lang, text_encoder, text_decoder, max_seq_length)\n",
    "        if report:\n",
    "            print(\"> %s\"%(readable_from_tokens(in_sentence_tokens)))\n",
    "            print(\"= %s\"%(readable_from_tokens(out_sentence_tokens)))\n",
    "            print(\"< %s\"%readable_from_tokens(translation_tokens[:-1]))\n",
    "            print(\"---\")\n",
    "        \n",
    "        yield [out_sentence_tokens], translation_tokens[:-1]\n",
    "        \n",
    "testing_set_bleu, sentence_count = compute_bleu(text_set_translation_generator(stop_at=100, report=True), max_order=4, smooth=False)\n",
    "\n",
    "print(\"Evaluated %i samples.\"%sentence_count)\n",
    "print(\"BLEU: \", testing_set_bleu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Text -> text on audio data\n",
    "gold_sentence_tokens = []\n",
    "predicted_sentence_tokens = []\n",
    "def audio_set_text_translation_generator(evaluation_set, stop_at = -1, report=False, model='text'):\n",
    "    for in_sentence_tokens, in_prosody_tokens , out_sentence_tokens, out_prosody_tokens, in_csv, out_csv in audio_data_generator(evaluation_set, input_lang, output_lang, n_prosody_params, input_prosody_params, output_prosody_params, dummyfy_input_prosody=True, dummyfy_output_prosody=True, stop_at=stop_at):\n",
    "        if model == 'text':\n",
    "            translation_tokens, _ = evaluate_text(in_sentence_tokens, input_lang, output_lang, text_encoder, text_decoder, max_seq_length)\n",
    "        elif model == 'audio':\n",
    "            translation_tokens, _, _ = evaluate_audio(in_sentence_tokens, in_prosody_tokens, input_lang, output_lang, prosodic_encoder, prosodic_decoder, max_seq_length, AUDIO_ENCODE_ONLY)\n",
    "        if report:\n",
    "            print(\"> %s\"%(readable_from_tokens(in_sentence_tokens)))\n",
    "            print(\"= %s\"%(readable_from_tokens(out_sentence_tokens)))\n",
    "            print(\"< %s\"%readable_from_tokens(translation_tokens[:-1]))\n",
    "            print(\"---\")\n",
    "            \n",
    "        gold_sentence_tokens.append(out_sentence_tokens)\n",
    "        predicted_sentence_tokens.append(translation_tokens[:-1])\n",
    "        yield [out_sentence_tokens], translation_tokens[:-1]\n",
    "             \n",
    "evaluation_set = AUDIO_TEST_DATA_PATH\n",
    "testing_set_bleu, sentence_count = compute_bleu(\n",
    "    audio_set_text_translation_generator(evaluation_set, report=False, stop_at=-1, model='text'), \n",
    "    max_order=4, smooth=False)\n",
    "\n",
    "print(\"Evaluated %i samples.\"%sentence_count)\n",
    "print(\"BLEU: \", testing_set_bleu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BLEU calculation on OpenNMT results\n",
    "predictions_file = \"/Users/alp/phdCloud/playground/OpenNMT-py/heroes_test_v2-pred.txt\"\n",
    "with open(predictions_file) as f:\n",
    "    content = f.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "openNMT_predictions = [x.strip() for x in content]\n",
    "\n",
    "def openNMT_translation_generator():\n",
    "    for gold, pred in zip(gold_sentence_tokens, openNMT_predictions):\n",
    "        print([gold])\n",
    "        print(pred.split(\" \"))\n",
    "        yield [gold], pred.split(\" \")\n",
    "        \n",
    "openNMT_bleu, sentence_count = compute_bleu(openNMT_translation_generator(), max_order=4, smooth=False)\n",
    "\n",
    "print(\"Evaluated %i samples.\"%sentence_count)\n",
    "print(\"BLEU: \", openNMT_bleu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create text translation data from compiled heroes data\n",
    "output_file = \"/Users/alp/phdCloud/playground/punkHeroes/out_heroes_v1/es_goldpuncd.txt\"\n",
    "\n",
    "input_lang.punctuation_level = 2\n",
    "output_lang.punctuation_level = 2\n",
    "\n",
    "with open(output_file, 'w') as f:\n",
    "    for in_sentence_tokens, _ , out_sentence_tokens, _, _, _ in audio_data_generator(AUDIO_TEST_DATA_PATH, input_lang, output_lang, n_prosody_params, input_prosody_params, output_prosody_params):\n",
    "        #to write tab separated en-es\n",
    "        #f.write(\"%s\\t%s\\n\"%(readable_from_tokens(in_sentence_tokens), readable_from_tokens(out_sentence_tokens)))\n",
    "        #to write only en\n",
    "        #f.write(\"%s\\n\"%(readable_from_tokens(in_sentence_tokens)))\n",
    "        #to write only es\n",
    "        f.write(\"%s\\n\"%(readable_from_tokens(out_sentence_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated 542 samples.\n",
      "BLEU:  0.16735539889325204\n"
     ]
    }
   ],
   "source": [
    "#BLEU calculation from already translated text files\n",
    "gold_file = \"/Users/alp/phdCloud/playground/punkHeroes/out_heroes_v1/es_goldpuncd.txt\"\n",
    "predictions_file = \"/Users/alp/phdCloud/playground/punkHeroes/out_heroes_v1/translated/en_punkProsed_ted_w_transProsed_5mmheroes_unpuncdinput_v1testset.txt\"\n",
    "\n",
    "def textfile_translation_generator(gold_file, predictions_file):\n",
    "    gold_sentences = []\n",
    "    predicted_sentences = []\n",
    "    with open(gold_file, 'r') as f:\n",
    "        for line in f:\n",
    "            gold_sentences.append(line.strip().split(\" \"))\n",
    "    with open(predictions_file, 'r') as f:\n",
    "        for line in f:\n",
    "            predicted_sentences.append(line.strip().split(\" \"))\n",
    "    for gold, pred in zip(gold_sentences, predicted_sentences):       \n",
    "        yield [gold], pred\n",
    "        \n",
    "bleu, sentence_count = compute_bleu(textfile_translation_generator(gold_file, predictions_file), max_order=4, smooth=False)\n",
    "\n",
    "print(\"Evaluated %i samples.\"%sentence_count)\n",
    "print(\"BLEU: \", bleu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu2cpu: True\n"
     ]
    }
   ],
   "source": [
    "#Initialize text models COPY\n",
    "text_encoder_path = 'models/5mmheroes_puncdinput_encoder.model'\n",
    "text_decoder_path = 'models/5mmheroes_puncdinput_decoder.model'\n",
    "\n",
    "text_encoder = GenericEncoder(input_lang.vocabulary_size, hidden_size, input_lang.get_weights_matrix(), n_layers)\n",
    "text_decoder = LuongAttnDecoderRNN(attn_model, hidden_size, output_lang.get_weights_matrix(), output_lang.vocabulary_size, n_layers, input_feed=config['DECODER_INPUT_FEED'])\n",
    "load_model(text_encoder, text_decoder, text_encoder_path, text_decoder_path, options.gpu2cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unknown (en): annapura\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unknown (en): deﬁne\n",
      "Unknown (en): andos\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "#Transprosing text files\n",
    "input_file = \"/Users/alp/phdCloud/playground/punkHeroes/out_heroes_v1/en_punkProsed_ted_w.txt\"\n",
    "gold_file = \"/Users/alp/phdCloud/playground/punkHeroes/out_heroes_v1/es_goldpuncd.txt\"\n",
    "output_file = \"/Users/alp/phdCloud/playground/punkHeroes/out_heroes_v1/translated/en_punkProsed_ted_w_transProsed_5mmheroes_unpuncdinput_v1testset.txt\"\n",
    "\n",
    "stop_at = -1\n",
    "report = False\n",
    "\n",
    "gold_sentences = []\n",
    "predicted_sentences = []\n",
    "input_sentences = []\n",
    "\n",
    "#read files\n",
    "with open(gold_file, 'r') as f:\n",
    "    for line in f:\n",
    "        gold_sentences.append(line.strip().split(\" \"))\n",
    "with open(input_file, 'r') as f:\n",
    "    for line in f:\n",
    "        input_sentences.append(line.strip().split(\" \"))\n",
    "#translate\n",
    "count = 0\n",
    "for in_sentence_tokens, gold_tokens in zip(input_sentences, gold_sentences):\n",
    "    translation_tokens, _ = evaluate_text(in_sentence_tokens, input_lang, output_lang, text_encoder, text_decoder, max_seq_length)\n",
    "    predicted_sentences.append(translation_tokens)\n",
    "    if report:\n",
    "        print(\"> %s\"%(readable_from_tokens(in_sentence_tokens)))\n",
    "        print(\"= %s\"%(readable_from_tokens(gold_tokens)))\n",
    "        print(\"< %s\"%readable_from_tokens(translation_tokens[:-1]))\n",
    "        print(\"---\")\n",
    "    #yield [gold_tokens], translation_tokens\n",
    "\n",
    "    count += 1\n",
    "    if count == stop_at:\n",
    "        break\n",
    "    if count % 100 == 0:\n",
    "        print(count)\n",
    "            \n",
    "#store translations in a text file\n",
    "with open(output_file, 'w') as f:\n",
    "    for token_index, script_tokens in enumerate(predicted_sentences):\n",
    "        f.write(\"%s\\n\" % ' '.join(script_tokens[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.6",
   "language": "python",
   "name": "python3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
