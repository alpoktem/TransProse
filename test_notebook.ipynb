{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from utils import *\n",
    "from models import *\n",
    "import yaml\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Options:\n",
    "    def __init__(self):\n",
    "        self.model_name = 'alp'\n",
    "\n",
    "options = Options()\n",
    "options.output_file = 'test_output/reapos_test_text.txt'\n",
    "options.params_file = 'params_joke.yaml'\n",
    "options.use_cuda = False\n",
    "options.use_validation = True\n",
    "options.gpu2cpu = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-13 13:10:34,174 : INFO : loading Word2Vec object from /Users/alp/Documents/Corpora/OpenSubtitles2018/w2v/reapos_min5_heroes/w2v_en_heroes.model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use cuda: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-13 13:10:36,406 : INFO : loading wv recursively from /Users/alp/Documents/Corpora/OpenSubtitles2018/w2v/reapos_min5_heroes/w2v_en_heroes.model.wv.* with mmap=None\n",
      "2018-08-13 13:10:36,407 : INFO : loading vectors from /Users/alp/Documents/Corpora/OpenSubtitles2018/w2v/reapos_min5_heroes/w2v_en_heroes.model.wv.vectors.npy with mmap=None\n",
      "2018-08-13 13:10:36,533 : INFO : setting ignored attribute vectors_norm to None\n",
      "2018-08-13 13:10:36,537 : INFO : loading vocabulary recursively from /Users/alp/Documents/Corpora/OpenSubtitles2018/w2v/reapos_min5_heroes/w2v_en_heroes.model.vocabulary.* with mmap=None\n",
      "2018-08-13 13:10:36,540 : INFO : loading trainables recursively from /Users/alp/Documents/Corpora/OpenSubtitles2018/w2v/reapos_min5_heroes/w2v_en_heroes.model.trainables.* with mmap=None\n",
      "2018-08-13 13:10:36,543 : INFO : loading syn1neg from /Users/alp/Documents/Corpora/OpenSubtitles2018/w2v/reapos_min5_heroes/w2v_en_heroes.model.trainables.syn1neg.npy with mmap=None\n",
      "2018-08-13 13:10:36,672 : INFO : setting ignored attribute cum_table to None\n",
      "2018-08-13 13:10:36,675 : INFO : loaded /Users/alp/Documents/Corpora/OpenSubtitles2018/w2v/reapos_min5_heroes/w2v_en_heroes.model\n",
      "2018-08-13 13:10:37,416 : INFO : loading Word2Vec object from /Users/alp/Documents/Corpora/OpenSubtitles2018/w2v/reapos_min5_heroes/w2v_es_heroes.model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en Vocabulary size: 30000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-13 13:10:39,614 : INFO : loading wv recursively from /Users/alp/Documents/Corpora/OpenSubtitles2018/w2v/reapos_min5_heroes/w2v_es_heroes.model.wv.* with mmap=None\n",
      "2018-08-13 13:10:39,615 : INFO : loading vectors from /Users/alp/Documents/Corpora/OpenSubtitles2018/w2v/reapos_min5_heroes/w2v_es_heroes.model.wv.vectors.npy with mmap=None\n",
      "2018-08-13 13:10:39,826 : INFO : setting ignored attribute vectors_norm to None\n",
      "2018-08-13 13:10:39,841 : INFO : loading vocabulary recursively from /Users/alp/Documents/Corpora/OpenSubtitles2018/w2v/reapos_min5_heroes/w2v_es_heroes.model.vocabulary.* with mmap=None\n",
      "2018-08-13 13:10:39,856 : INFO : loading trainables recursively from /Users/alp/Documents/Corpora/OpenSubtitles2018/w2v/reapos_min5_heroes/w2v_es_heroes.model.trainables.* with mmap=None\n",
      "2018-08-13 13:10:39,861 : INFO : loading syn1neg from /Users/alp/Documents/Corpora/OpenSubtitles2018/w2v/reapos_min5_heroes/w2v_es_heroes.model.trainables.syn1neg.npy with mmap=None\n",
      "2018-08-13 13:10:40,224 : INFO : setting ignored attribute cum_table to None\n",
      "2018-08-13 13:10:40,226 : INFO : loaded /Users/alp/Documents/Corpora/OpenSubtitles2018/w2v/reapos_min5_heroes/w2v_es_heroes.model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "es Vocabulary size: 30000\n"
     ]
    }
   ],
   "source": [
    "#LOAD CONFIGURATIONS AND LANGUAGES\n",
    "USE_CUDA = options.use_cuda\n",
    "print(\"Use cuda: %s\" %USE_CUDA)\n",
    "\n",
    "try:\n",
    "    with open(options.params_file, 'r') as ymlfile:\n",
    "        config = yaml.load(ymlfile)\n",
    "except:\n",
    "    sys.exit(\"Parameters file missing\")\n",
    "\n",
    "#Setup languages\n",
    "INPUT_LANG_CODE = config['INPUT_LANG']\n",
    "OUTPUT_LANG_CODE = config['OUTPUT_LANG']\n",
    "\n",
    "INPUT_LANG_PUNC_LEVEL = config[\"INPUT_LANG_PUNC_LEVEL\"]\n",
    "OUTPUT_LANG_PUNC_LEVEL = config[\"OUTPUT_LANG_PUNC_LEVEL\"]\n",
    "\n",
    "if INPUT_LANG_CODE == 'en' and OUTPUT_LANG_CODE == 'es':\n",
    "    lang_en = input_lang = Lang(INPUT_LANG_CODE, config[\"W2V_EN_PATH\"], config[\"DICT_EN_PATH\"], punctuation_level=INPUT_LANG_PUNC_LEVEL)\n",
    "    lang_es = output_lang = Lang(OUTPUT_LANG_CODE, config[\"W2V_ES_PATH\"], config[\"DICT_ES_PATH\"], punctuation_level=OUTPUT_LANG_PUNC_LEVEL)\n",
    "elif INPUT_LANG_CODE == 'es' and OUTPUT_LANG_CODE == 'en':\n",
    "    lang_es = input_lang = Lang(INPUT_LANG_CODE, config[\"W2V_ES_PATH\"], config[\"DICT_ES_PATH\"], punctuation_level=INPUT_LANG_PUNC_LEVEL)\n",
    "    lang_en = output_lang = Lang(OUTPUT_LANG_CODE, config[\"W2V_EN_PATH\"], config[\"DICT_EN_PATH\"], punctuation_level=OUTPUT_LANG_PUNC_LEVEL)\n",
    "\n",
    "input_prosody_params = config['INPUT_PROSODY']\n",
    "if input_prosody_params == None:\n",
    "    input_prosody_params = []\n",
    "output_prosody_params = config['OUTPUT_PROSODY']\n",
    "if output_prosody_params == None:\n",
    "    output_prosody_params = []    \n",
    "    \n",
    "#NETWORK CONFIG\n",
    "max_seq_length = int(config['MAX_SEQ_LENGTH'])\n",
    "n_prosody_params = int(config['N_PROSODY_PARAMS'])\n",
    "input_prosody_params = config['INPUT_PROSODY']\n",
    "encoder_type = config['ENCODER_TYPE']\n",
    "attn_model = config['ATTN_MODEL']\n",
    "hidden_size = int(config['HIDDEN_SIZE'])\n",
    "n_layers = int(config['N_LAYERS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD DATASETS    \n",
    "AUDIO_TEST_DATA_PATH = config[\"AUDIO_TEST_DATA_FILE\"]\n",
    "AUDIO_TRAIN_DATA_PATH = config[\"AUDIO_TRAIN_DATA_FILE\"]\n",
    "AUDIO_VALIDATION_DATA_PATH = config[\"AUDIO_VALIDATION_DATA_FILE\"]\n",
    "TEXT_TEST_DATA_PATH = config['TEXT_TEST_DATA_PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input punc lvl:  0\n",
      "Output punc lvl:  2\n",
      "gpu2cpu: True\n"
     ]
    }
   ],
   "source": [
    "#Initialize text models\n",
    "text_encoder_path = 'models/5mmheroes_unpuncdinput_encoder.model'\n",
    "text_decoder_path = 'models/5mmheroes_unpuncdinput_decoder.model'\n",
    "\n",
    "print('Input punc lvl: ', INPUT_LANG_PUNC_LEVEL)\n",
    "print('Output punc lvl: ', OUTPUT_LANG_PUNC_LEVEL)\n",
    "\n",
    "text_encoder = GenericEncoder(input_lang.vocabulary_size, hidden_size, input_lang.get_weights_matrix(), n_layers)\n",
    "text_decoder = LuongAttnDecoderRNN(attn_model, hidden_size, output_lang.get_weights_matrix(), output_lang.vocabulary_size, n_layers, input_feed=config['DECODER_INPUT_FEED'])\n",
    "load_model(text_encoder, text_decoder, text_encoder_path, text_decoder_path, options.gpu2cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu2cpu: True\n"
     ]
    }
   ],
   "source": [
    "#Initialize prosodic models\n",
    "AUDIO_ENCODE_ONLY = False\n",
    "model_name = \"audio_pausein_dummyout_miniset\"\n",
    "prosodic_encoder_path = 'models/' + model_name + '_encoder.model'\n",
    "prosodic_decoder_path = 'models/' + model_name + '_decoder.model'\n",
    "\n",
    "if encoder_type == 'sum':\n",
    "    prosodic_encoder = EncoderRNN_sum(input_lang.vocabulary_size, n_prosody_params, hidden_size, input_lang.get_weights_matrix(), n_layers)\n",
    "elif encoder_type == 'parallel':\n",
    "    prosodic_encoder = EncoderRNN_parallel(input_lang.vocabulary_size, n_prosody_params, hidden_size, input_lang.get_weights_matrix(), n_layers)\n",
    "else:\n",
    "    sys.exit(\"Unrecognized encoder type. Check params file. Exiting...\")\n",
    "if AUDIO_ENCODE_ONLY:\n",
    "    prosodic_decoder = LuongAttnDecoderRNN(attn_model, hidden_size, output_lang.get_weights_matrix(), output_lang.vocabulary_size, n_layers, USE_CUDA=USE_CUDA)\n",
    "else:\n",
    "    prosodic_decoder = ProsodicDecoderRNN(attn_model, hidden_size, output_lang.get_weights_matrix(), output_lang.vocabulary_size, n_layers, USE_CUDA=USE_CUDA)\n",
    "really_prosodic_decoder = ProsodicDecoderRNN(attn_model, hidden_size, output_lang.get_weights_matrix(), output_lang.vocabulary_size, n_layers, USE_CUDA=USE_CUDA)\n",
    "    \n",
    "load_model(prosodic_encoder, prosodic_decoder, prosodic_encoder_path, prosodic_decoder_path, gpu_to_cpu=options.gpu2cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(prosodic_encoder.state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA GENERATORS\n",
    "def text_data_generator(data_path, input_lang, output_lang, stop_at=-1):\n",
    "    count = 0\n",
    "    with open(data_path,'r') as inputfile:\n",
    "        for line in inputfile:\n",
    "            if not stop_at == -1 and count >= stop_at:\n",
    "                break\n",
    "            pair = [sentence.strip() for sentence in line.split('\\t')]\n",
    "            if input_lang.lang_code == 'en':\n",
    "                in_sentence = pair[0]\n",
    "                out_sentence = pair[1]\n",
    "            elif input_lang.lang_code == 'es':\n",
    "                in_sentence = pair[1]\n",
    "                out_sentence = pair[0]\n",
    "\n",
    "            in_sentence_tokens = in_sentence.lower().split()\n",
    "            out_sentence_tokens = out_sentence.lower().split()\n",
    "\n",
    "            if input_lang.punctuation_level == 0:\n",
    "                in_sentence_tokens = remove_punc_tokens(in_sentence_tokens)\n",
    "            elif input_lang.punctuation_level == 1:\n",
    "                in_sentence_tokens = remove_punc_tokens(in_sentence_tokens, keep_main_puncs=True)\n",
    "            if output_lang.punctuation_level == 0:\n",
    "                out_sentence_tokens = remove_punc_tokens(out_sentence_tokens)\n",
    "            elif output_lang.punctuation_level == 1:\n",
    "                out_sentence_tokens = remove_punc_tokens(out_sentence_tokens, keep_main_puncs=True)\n",
    "\n",
    "            count += 1\n",
    "\n",
    "            yield in_sentence_tokens, out_sentence_tokens\n",
    "            \n",
    "\n",
    "def audio_data_generator(data_path, input_lang, output_lang, n_prosody_params, input_prosody_params, output_prosody_params, dummyfy_input_prosody=False, dummyfy_output_prosody=False, stop_at = -1):\n",
    "    assert not input_lang == output_lang\n",
    "    audio_data = read_audio_dataset_file(data_path, shuffle=False)\n",
    "\n",
    "    #start generating samples from the proscript links in the data file\n",
    "    count = 0\n",
    "    for segment_data in audio_data:\n",
    "        if not stop_at == -1 and count >= stop_at:\n",
    "            break\n",
    "\n",
    "        es_txt = segment_data[0]\n",
    "        es_csv = segment_data[1]\n",
    "        en_txt = segment_data[2]\n",
    "        en_csv = segment_data[3]\n",
    "        \n",
    "        #print(en_csv)\n",
    "        #print(es_csv)\n",
    "\n",
    "        if input_lang.lang_code == 'en' and output_lang.lang_code == 'es':\n",
    "            input_proscript = en_csv\n",
    "            output_proscript = es_csv\n",
    "            #input_transcript = read_text_file(en_txt)\n",
    "            #output_transcript = read_text_file(es_txt)\n",
    "        elif input_lang.lang_code == 'es' and output_lang.lang_code == 'en':\n",
    "            input_proscript = es_csv\n",
    "            output_proscript = en_csv\n",
    "            #input_transcript = read_text_file(es_txt)\n",
    "            #output_transcript = read_text_file(en_txt)\n",
    "            \n",
    "        if input_lang.punctuation_level == 0:\n",
    "            input_punc = False\n",
    "            input_only_main_punc = False\n",
    "        elif input_lang.punctuation_level == 1:\n",
    "            input_punc = True\n",
    "            input_only_main_punc = True\n",
    "        elif input_lang.punctuation_level == 2:\n",
    "            input_punc = True\n",
    "            input_only_main_punc = False\n",
    "            \n",
    "        if output_lang.punctuation_level == 0:\n",
    "            output_punc = False\n",
    "            output_only_main_punc = False\n",
    "        elif output_lang.punctuation_level == 1:\n",
    "            output_punc = True\n",
    "            output_only_main_punc = True\n",
    "        elif output_lang.punctuation_level == 2:\n",
    "            output_punc = True\n",
    "            output_only_main_punc = False\n",
    "\n",
    "        in_sentence_tokens, in_prosody_tokens = read_data_from_proscript(input_proscript, input_lang, n_prosody_params, input_prosody_params, punctuation_as_tokens = input_punc, keep_only_main_puncs = input_only_main_punc)\n",
    "        out_sentence_tokens, out_prosody_tokens = read_data_from_proscript(output_proscript, output_lang, n_prosody_params, output_prosody_params, punctuation_as_tokens = output_punc, keep_only_main_puncs = output_only_main_punc)\n",
    "    \n",
    "        if dummyfy_input_prosody:\n",
    "            in_prosody_tokens = np.zeros_like(in_prosody_tokens)\n",
    "        if dummyfy_output_prosody:\n",
    "            out_prosody_tokens = np.zeros_like(out_prosody_tokens)\n",
    "            \n",
    "        count += 1\n",
    "        yield in_sentence_tokens, in_prosody_tokens, out_sentence_tokens, out_prosody_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#various utilities\n",
    "def print_prosody(prosody):\n",
    "    print(np.array(prosody).transpose())\n",
    "    \n",
    "def print_tokens_with_pause(tokens, pausevals = [], pauseflags=[]):\n",
    "    if pauseflags == [] and not pausevals == []:\n",
    "        pauseflags = flags_from_value(pausevals)\n",
    "        \n",
    "    to_print = \"\"\n",
    "    for i, token in enumerate(tokens):\n",
    "        to_print += token + \" \"\n",
    "        #if not pausevals == [] and pauseflags[i]:\n",
    "        if not pausevals == []:\n",
    "            to_print += \"[\" + \"{:.2f}\".format(pausevals[i]) + \"]\"\n",
    "            if pauseflags[i]:\n",
    "                to_print += \"[1]\"\n",
    "            to_print += \" \"\n",
    "            \n",
    "    print(to_print)\n",
    "    return to_print\n",
    "            \n",
    "def flags_from_value(prosody_seq):\n",
    "    return [1 if not feature_value == 0.0 else 0 for feature_value in prosody_seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SEE TEXT DATA\n",
    "for in_sent, out_sent in text_data_generator(TEXT_TEST_DATA_PATH, input_lang, output_lang):\n",
    "    print(in_sent)\n",
    "    print(out_sent)\n",
    "    exit = input('...')\n",
    "    if exit == 'q':\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#SEE AUDIO DATA\n",
    "in_pros = []\n",
    "for in_sent, in_pros, out_sent, out_pros in audio_data_generator(AUDIO_TEST_DATA_PATH, input_lang, output_lang, n_prosody_params, input_prosody_params, output_prosody_params, dummyfy_input_prosody=False, dummyfy_output_prosody=False):\n",
    "    print(in_sent)\n",
    "    print(indexes_from_tokens(input_lang, in_sent))\n",
    "    in_pros = finalize_prosody_sequence(in_pros)\n",
    "    print_prosody(in_pros)\n",
    "    print(out_sent)\n",
    "    print_prosody(out_pros)\n",
    "    exit = input('...')\n",
    "    if exit == 'q':\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EVALUATORS\n",
    "#text -> text\n",
    "def evaluate_text(input_seq_tokens, input_lang, output_lang, encoder, decoder, max_length, USE_CUDA=False):\n",
    "    input_word_seqs = [indexes_from_tokens(input_lang, input_seq_tokens)]\n",
    "    \n",
    "    #make sure sequences are below max_length. \n",
    "    input_word_seqs = limit_seqs_to_max(input_word_seqs, max_length)\n",
    "\n",
    "    input_lengths = [len(input_word_seqs[0])]\n",
    "    input_word_batch = Variable(torch.LongTensor(input_word_seqs)).transpose(0, 1)\n",
    "\n",
    "    if USE_CUDA:\n",
    "        input_word_batch = input_batch.cuda()\n",
    "        input_prosody_batch = input_prosody_batch.cuda()\n",
    "\n",
    "    # Set to not-training mode to disable dropout\n",
    "    encoder.train(False)\n",
    "    decoder.train(False)\n",
    "\n",
    "    # Run through encoder\n",
    "    encoder_outputs, encoder_hidden = encoder(input_word_batch, input_lengths, None)\n",
    "\n",
    "    # Create starting vectors for decoder\n",
    "    decoder_input = Variable(torch.LongTensor([output_lang.token2index(SWT_TOKEN)])) # SOS\n",
    "    decoder_hidden = encoder_hidden[:decoder.n_layers] # Use last (forward) hidden state from encoder\n",
    "    decoder_context = Variable(torch.zeros(1, decoder.hidden_size))\n",
    "\n",
    "    if USE_CUDA:\n",
    "        decoder_input = decoder_input.cuda()\n",
    "        decoder_context = decoder_context.cuda()\n",
    "\n",
    "    # Store output words and attention states\n",
    "    decoded_words = []\n",
    "    decoder_attentions = torch.zeros(max_length + 1, max_length + 1)\n",
    "\n",
    "    # Run through decoder\n",
    "    for di in range(max_length):\n",
    "        decoder_output, decoder_context, decoder_hidden, decoder_attn = decoder(\n",
    "                decoder_input, decoder_context, decoder_hidden, encoder_outputs)\n",
    "        \n",
    "        decoder_attentions[di,:decoder_attn.size(2)] += decoder_attn.squeeze(0).squeeze(0).cpu().data\n",
    "\n",
    "        # Choose top word from output\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        #ni = topi[0][0]  #old code\n",
    "        ni = topi.item()\n",
    "        if ni == output_lang.token2index(EOS_TOKEN):\n",
    "            decoded_words.append(EOS_TOKEN)\n",
    "            break\n",
    "        else:\n",
    "            decoded_words.append(output_lang.index2token(ni))\n",
    "\n",
    "        # Next input is chosen word\n",
    "        decoder_input = Variable(torch.LongTensor([ni]))\n",
    "        if USE_CUDA: decoder_input = decoder_input.cuda()\n",
    "\n",
    "    # Set back to training mode\n",
    "    encoder.train(True)\n",
    "    decoder.train(True)\n",
    "\n",
    "    return decoded_words, decoder_attentions[:di+1, :len(encoder_outputs)]\n",
    "\n",
    "#text+audio -> text+audio\n",
    "#use prosodic encoder/decoder\n",
    "def evaluate_audio(input_seq_tokens, input_prosody_tokens, input_lang, output_lang, encoder, decoder, max_length, audio_encode_only=AUDIO_ENCODE_ONLY, USE_CUDA=False):\n",
    "    input_word_seqs = [indexes_from_tokens(input_lang, input_seq_tokens)]\n",
    "    input_prosody_seqs = [finalize_prosody_sequence(input_prosody_tokens)] #put the end token\n",
    "    \n",
    "    #make sure sequences are below max_length. \n",
    "    input_word_seqs = limit_seqs_to_max(input_word_seqs, max_length)\n",
    "    input_prosody_seqs = limit_seqs_to_max(input_prosody_seqs, max_length)\n",
    "\n",
    "    input_lengths = [len(input_word_seqs[0])]\n",
    "    input_word_batch = Variable(torch.LongTensor(input_word_seqs)).transpose(0, 1)\n",
    "    input_prosody_batch = Variable(torch.FloatTensor(input_prosody_seqs)).transpose(0, 1)\n",
    "    \n",
    "    if USE_CUDA:\n",
    "        input_word_batch = input_batch.cuda()\n",
    "        input_prosody_batch = input_prosody_batch.cuda()\n",
    "\n",
    "    # Set to not-training mode to disable dropout\n",
    "    encoder.train(False)\n",
    "    decoder.train(False)\n",
    "\n",
    "    # Run through encoder\n",
    "    encoder_outputs, encoder_hidden = encoder(input_word_batch, input_prosody_batch, input_lengths, None)\n",
    "\n",
    "    # Create starting vectors for decoder\n",
    "    decoder_word_input = Variable(torch.LongTensor([output_lang.token2index(SWT_TOKEN)])) # SOS\n",
    "    decoder_hidden = encoder_hidden[:decoder.n_layers] # Use last (forward) hidden state from encoder\n",
    "    decoder_context = Variable(torch.zeros(1, decoder.hidden_size))\n",
    "\n",
    "    if USE_CUDA:\n",
    "        decoder_word_input = decoder_word_input.cuda()\n",
    "        decoder_context = decoder_context.cuda()\n",
    "\n",
    "    # Store output words and attention states\n",
    "    decoded_word_seq = []\n",
    "    decoded_pauseflag_seq = []\n",
    "    decoded_pausevalue_seq = []\n",
    "    decoder_attentions = torch.zeros(max_length + 1, max_length + 1)\n",
    "\n",
    "    # Run through decoder\n",
    "    decoder_stop = False\n",
    "    for di in range(max_length):\n",
    "        if audio_encode_only:\n",
    "            decoder_output_word, decoder_context, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_word_input, decoder_context, decoder_hidden, encoder_outputs)\n",
    "        else:\n",
    "            decoder_output_word, decoder_output_pauseflag, decoder_output_pausevalue, decoder_context, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_word_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "        \n",
    "        decoder_attentions[di,:decoder_attention.size(2)] += decoder_attention.squeeze(0).squeeze(0).cpu().data\n",
    "            \n",
    "        # Choose top word from output\n",
    "        topv, topi = decoder_output_word.data.topk(1)\n",
    "        ni_word = topi.item()\n",
    "        if ni_word == output_lang.token2index(EOS_TOKEN):\n",
    "            decoded_word_seq.append(EOS_TOKEN)\n",
    "            #decoder_stop = True\n",
    "        else:\n",
    "            decoded_word_seq.append(output_lang.index2token(ni_word))\n",
    "        \n",
    "        if not audio_encode_only:\n",
    "            # Look at pauseflag output\n",
    "            topv, topi = decoder_output_pauseflag.data.topk(1)\n",
    "            ni_pauseflag = topi.item()\n",
    "            decoded_pauseflag_seq.append(ni_pauseflag)\n",
    "\n",
    "            # Look at pauseval output\n",
    "            predicted_pausevalue = decoder_output_pausevalue.item()\n",
    "            #decoded_pausevalue_seq.append(unnormalize_value(predicted_pausevalue, 0.0, 10.0))\n",
    "            decoded_pausevalue_seq.append(predicted_pausevalue)\n",
    "\n",
    "        # Next input is chosen word\n",
    "        if decoder_stop:\n",
    "            break\n",
    "        else:\n",
    "            decoder_word_input = Variable(torch.LongTensor([ni_word]))\n",
    "            if USE_CUDA: decoder_word_input = decoder_word_input.cuda()\n",
    "\n",
    "    # Set back to training mode\n",
    "    encoder.train(True)\n",
    "    decoder.train(True)\n",
    "\n",
    "    return decoded_word_seq, decoded_pauseflag_seq, decoded_pausevalue_seq, decoder_attentions[:di+1, :len(encoder_outputs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he encontrado a los otros siete .\n"
     ]
    }
   ],
   "source": [
    "#Text -> text evaluator test\n",
    "input_sentence = \"i found the other seven yesterday\" \n",
    "input_seq_tokens = input_sentence.split()\n",
    "decoded_words, _ = evaluate_text(input_seq_tokens, input_lang, output_lang, text_encoder, text_decoder, max_seq_length)\n",
    "print(readable_from_tokens(decoded_words[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = audio_data_generator(AUDIO_TRAIN_DATA_PATH, input_lang, output_lang, n_prosody_params, input_prosody_params, output_prosody_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN\n",
      "i [0.00] am [0.18][1] shutting [0.00] this [0.00] operation [0.05][1] down [0.54][1] today [0.38][1] and [0.00] then [0.00] i [0.00] 'm [0.00] going [0.00] to [0.00] the [0.03][1] ag [0.00] 's [0.00] office [0.14][1] and [0.00] i [0.00] 'm [0.00] filing [0.00] human [0.00] rights [0.00] violations [0.00] against [0.00] you [0.00] senator [0.00] \n",
      "GT\n",
      "voy [0.00] a [0.00] cerrar [0.03][1] este [0.03][1] proyecto [0.41][1] hoy [0.20][1] mismo [0.51][1] . [0.00] luego [0.09][1] iré [0.00] al [0.00] fiscal [0.00] general [0.06][1] ... [0.00] y [0.00] le [0.00] demandaré [0.00] por [0.00] violación [0.00] de [0.00] los [0.00] derechos [0.00] humanos [0.00] , [0.00] senador [0.00] . [0.00] \n",
      "OUT PROSODY\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alp/extSW/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel/__main__.py:6: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n",
      "/Users/alp/extSW/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel/__main__.py:13: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voy [0.04] a [0.04] cerrar [0.04] este [0.04] proyecto [0.04] hoy [0.04] mismo [0.04] . [0.04] luego [0.04] iré [0.04] al [0.04] fiscal [0.04] general [0.04] ... [0.04] y [0.04] le [0.04] demandaré [0.04] por [0.04] violación [0.04] . [0.04] END [0.04] , [0.03] senador [0.03] . [0.04] END [0.03] , [0.03] senador [0.04] . [0.04] END [0.03] , [0.03] senador [0.04] . [0.04] END [0.03] , [0.03] senador [0.04] . [0.04] END [0.03] , [0.03] senador [0.04] . [0.04] \n",
      "OUT TEXT\n",
      "voy a cerrar esta investigación , y luego voy a ir a la oficina de la oficina de la oficina . END \n",
      "...\n",
      "======================================================================\n",
      "IN\n",
      "do [0.00] i [0.00] have [0.00] to [0.00] choose [0.00] \n",
      "GT\n",
      "¿ [0.00] tengo [0.00] que [0.00] elegir [0.00] ? [0.00] \n",
      "OUT PROSODY\n",
      "¿ [0.04] tengo [0.04] que [0.04] elegir [0.04] ? [0.04] END [0.04] . [0.04] END [0.04] . [0.04] END [0.04] . [0.04] END [0.04] . [0.04] END [0.04] . [0.04] END [0.04] . [0.04] END [0.04] . [0.04] END [0.04] . [0.04] END [0.04] . [0.04] END [0.04] . [0.04] END [0.04] . [0.04] END [0.04] . [0.04] END [0.04] . [0.04] END [0.04] . [0.04] END [0.04] . [0.04] END [0.04] . [0.04] END [0.04] . [0.04] END [0.04] \n",
      "OUT TEXT\n",
      "¿ tengo que elegir ? END \n",
      "...\n",
      "======================================================================\n",
      "IN\n",
      "you [0.00] can [0.00] 't [0.00] trust [0.00] her [0.00] matt [0.39][1] she [0.00] lies [0.00] they [0.00] all [0.00] lie [0.03][1] send [0.00] her [0.00] away [0.00] \n",
      "GT\n",
      "no [0.00] te [0.00] fíes [0.00] de [0.00] ella [0.00] , [0.00] matt [0.47][1] . [0.00] miente [0.34][1] . [0.00] todas [0.00] mienten [0.03][1] . [0.00] haz [0.07][1] que [0.00] se [0.03][1] vaya [0.00] . [0.00] \n",
      "OUT PROSODY\n",
      "no [0.04] te [0.04] fíes [0.04] de [0.04] ella [0.04] , [0.04] matt [0.04] . [0.04] miente [0.04] . [0.04] todas [0.04] mienten [0.04] . [0.03] haz [0.03] . [0.04] haz [0.03] . [0.03] haz [0.03] . [0.03] haz [0.03] . [0.03] haz [0.03] . [0.03] haz [0.03] . [0.03] haz [0.03] . [0.03] haz [0.03] . [0.03] haz [0.03] . [0.03] END [0.03] . [0.04] END [0.04] . [0.04] END [0.04] . [0.04] END [0.04] . [0.04] END [0.04] \n",
      "OUT TEXT\n",
      "no puedes fiar en ella , matt . mienten . END \n",
      "...q\n"
     ]
    }
   ],
   "source": [
    "#Text+prosody -> text+prosody on audio data visualization\n",
    "print_every = 1\n",
    "print_count = 0\n",
    "for in_sentence_tokens, in_prosody_tokens , out_sentence_tokens, out_prosody_tokens in gen:\n",
    "    print_count += 1\n",
    "    print(\"IN\")\n",
    "    print_tokens_with_pause(in_sentence_tokens, in_prosody_tokens[:,0])\n",
    "    print(\"GT\")\n",
    "    print_tokens_with_pause(out_sentence_tokens, out_prosody_tokens[:,0])\n",
    "    print(\"OUT PROSODY\")\n",
    "    translation_tokens, pauseflag_tokens, pausevalue_tokens, _ = evaluate_audio(in_sentence_tokens, in_prosody_tokens, input_lang, output_lang, prosodic_encoder, prosodic_decoder, max_seq_length, audio_encode_only=AUDIO_ENCODE_ONLY)\n",
    "    print_tokens_with_pause(translation_tokens, pausevalue_tokens, pauseflag_tokens)\n",
    "    print(\"OUT TEXT\")\n",
    "    translation_tokens, _ = evaluate_text(in_sentence_tokens, input_lang, output_lang, text_encoder, text_decoder, max_seq_length)\n",
    "    print_tokens_with_pause(translation_tokens)\n",
    "    \n",
    "    if print_count % print_every == 0:\n",
    "        inp = input(\"...\")\n",
    "        if inp == 'q':\n",
    "            break\n",
    "    print(\"======================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text -> text on text data\n",
    "def text_set_translation_generator(stop_at = -1, report=False):\n",
    "    for in_sentence_tokens, out_sentence_tokens in text_data_generator(TEXT_TEST_DATA_PATH, input_lang, output_lang, stop_at):\n",
    "        translation_tokens, _ = evaluate_text(in_sentence_tokens, input_lang, output_lang, text_encoder, text_decoder, max_seq_length)\n",
    "        if report:\n",
    "            print(\"> %s\"%(readable_from_tokens(in_sentence_tokens)))\n",
    "            print(\"= %s\"%(readable_from_tokens(out_sentence_tokens)))\n",
    "            print(\"< %s\"%readable_from_tokens(translation_tokens[:-1]))\n",
    "            print(\"---\")\n",
    "        \n",
    "        yield [out_sentence_tokens], translation_tokens[:-1]\n",
    "        \n",
    "testing_set_bleu, sentence_count = compute_bleu(text_set_translation_generator(stop_at=100, report=True), max_order=4, smooth=False)\n",
    "\n",
    "print(\"Evaluated %i samples.\"%sentence_count)\n",
    "print(\"BLEU: \", testing_set_bleu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated 10 samples.\n",
      "BLEU:  0.0\n"
     ]
    }
   ],
   "source": [
    "#Text -> text on audio data\n",
    "def audio_set_text_translation_generator(stop_at = -1, report=False, model='text'):\n",
    "    for in_sentence_tokens, in_prosody_tokens , out_sentence_tokens, out_prosody_tokens in audio_data_generator(AUDIO_TEST_DATA_PATH, input_lang, output_lang, n_prosody_params, input_prosody_params, output_prosody_params, dummyfy_input_prosody=True, dummyfy_output_prosody=True, stop_at=stop_at):\n",
    "        if model == 'text':\n",
    "            translation_tokens, _ = evaluate_text(in_sentence_tokens, input_lang, output_lang, text_encoder, text_decoder, max_seq_length)\n",
    "        elif model == 'audio':\n",
    "            translation_tokens, _, _, _ = evaluate_audio(in_sentence_tokens, in_prosody_tokens, input_lang, output_lang, prosodic_encoder, prosodic_decoder, max_seq_length)\n",
    "        if report:\n",
    "            print(\"> %s\"%(readable_from_tokens(in_sentence_tokens)))\n",
    "            print(\"= %s\"%(readable_from_tokens(out_sentence_tokens)))\n",
    "            print(\"< %s\"%readable_from_tokens(translation_tokens[:-1]))\n",
    "            print(\"---\")\n",
    "        \n",
    "        yield [out_sentence_tokens], translation_tokens[:-1]\n",
    "                \n",
    "testing_set_bleu, sentence_count = compute_bleu(audio_set_text_translation_generator(report=False, stop_at=10, model='audio'), max_order=4, smooth=False)\n",
    "\n",
    "print(\"Evaluated %i samples.\"%sentence_count)\n",
    "print(\"BLEU: \", testing_set_bleu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create text translation data from compiled heroes data\n",
    "output_file = \"/Users/alp/Movies/heroes/transProse_data/trainable/heroes_trainable_test_text.txt\"\n",
    "\n",
    "with open(output_file, 'w') as f:\n",
    "    for in_sentence_tokens, _ , out_sentence_tokens, _ in audio_data_generator(AUDIO_TEST_DATA_PATH, input_lang, output_lang, n_prosody_params, input_prosody_params, output_prosody_params):\n",
    "        f.write(\"%s\\t%s\\n\"%(readable_from_tokens(in_sentence_tokens), readable_from_tokens(out_sentence_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 1.00000e-02 *torch.tensor(\n",
    "       [[[ 3.7000]],\n",
    "\n",
    "        [[ 3.7000]],\n",
    "\n",
    "        [[ 3.7000]],\n",
    "\n",
    "        [[ 3.1000]],\n",
    "\n",
    "        [[ 3.7000]],\n",
    "\n",
    "        [[ 3.7000]],\n",
    "\n",
    "        [[ 3.7000]],\n",
    "\n",
    "        [[ 3.7000]],\n",
    "\n",
    "        [[ 3.7000]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.03700000047683716,\n",
       " 0.03700000047683716,\n",
       " 0.03700000047683716,\n",
       " 0.03099999763071537,\n",
       " 0.03700000047683716,\n",
       " 0.03700000047683716,\n",
       " 0.03700000047683716,\n",
       " 0.03700000047683716,\n",
       " 0.03700000047683716]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.squeeze().tolist()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.03700000047683716,\n",
       " 0.03700000047683716,\n",
       " 0.03700000047683716,\n",
       " 0.03099999763071537,\n",
       " 0.03700000047683716,\n",
       " 0.03700000047683716,\n",
       " 0.03700000047683716,\n",
       " 0.03700000047683716,\n",
       " 0.03700000047683716,\n",
       " 0.03700000047683716,\n",
       " 0.03700000047683716,\n",
       " 0.03700000047683716,\n",
       " 0.03099999763071537,\n",
       " 0.03700000047683716,\n",
       " 0.03700000047683716,\n",
       " 0.03700000047683716,\n",
       " 0.03700000047683716,\n",
       " 0.03700000047683716]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#alll = []\n",
    "alll += a\n",
    "alll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.6",
   "language": "python",
   "name": "python3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
